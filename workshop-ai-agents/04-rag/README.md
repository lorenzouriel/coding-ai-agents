# üîç 04-RAG - Retrieval-Augmented Generation

Esta pasta cont√©m uma implementa√ß√£o educacional completa de **Retrieval-Augmented Generation (RAG)**, demonstrando cada etapa do processo desde o carregamento de documentos at√© a gera√ß√£o de respostas contextuais usando tanto **Ollama** (local) quanto **OpenAI**.

## üéØ Objetivo Educacional

Demonstrar na pr√°tica todos os conceitos fundamentais de RAG atrav√©s de exemplos progressivos, desde conceitos b√°sicos at√© implementa√ß√£o completa, focando em:
- **Pipeline RAG completo** passo-a-passo
- **Chunking inteligente** e suas implica√ß√µes
- **Embeddings vetoriais** e busca sem√¢ntica
- **Avalia√ß√£o de qualidade** com m√©tricas
- **Otimiza√ß√£o de performance** e debugging

## Estrutura dos Arquivos

### Pipeline Completo
| Arquivo | Descri√ß√£o | N√≠vel |
|---------|-----------|-------|
| **`RAG_pipeline.py`** | Sistema RAG completo e funcional | ‚≠ê‚≠ê‚≠ê **Principal** |

### Conceitos Detalhados (Step-by-Step)
| Arquivo | Conceito | Foco Educacional |
|---------|----------|------------------|
| **`chunking-example.py`** | Text Chunking | Como texto √© dividido e impactos |
| **`embedding-example.py`** | Vector Embeddings | Como texto vira n√∫meros |
| **`semantic-search-example.py`** | Busca Sem√¢ntica | Similaridade vs palavras-chave |
| **`context_enrichment.py`** | Context Enrichment | Prepara√ß√£o para gera√ß√£o |

### Dataset e Avalia√ß√£o
| Item | Descri√ß√£o |
|------|-----------|
| **`data/Understanding_Climate_Change.pdf`** | Documento exemplo para demonstra√ß√µes |
| **`db/`** | Bancos vetoriais (Chroma + FAISS) |

## Pipeline RAG Detalhado

### Step 1: Document Loading
```python
# Carregamento de PDF com metadados
loader = PyPDFLoader(file_path)
documents = loader.load()
```
**Conceitos:** Processamento de documentos, preserva√ß√£o de metadados

### Step 2: Text Chunking
```python
# Divis√£o inteligente com overlap
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Tamanho do chunk
    chunk_overlap=200,    # Sobreposi√ß√£o para contexto
    separators=["\n\n", "\n", " ", ""]  # Hierarquia de separa√ß√£o
)
```
**Conceitos:** Balanceamento contexto vs precis√£o, continuidade sem√¢ntica

### Step 3: Vector Embeddings
```python
# Convers√£o texto ‚Üí vetores
embeddings_model = OllamaEmbeddings(model="mxbai-embed-large")
vectorstore = Chroma.from_documents(documents, embeddings_model)
```
**Conceitos:** Representa√ß√£o sem√¢ntica, similaridade cosine

### Step 4: Semantic Search
```python
# Busca por similaridade
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Top 5 resultados
)
```
**Conceitos:** Relev√¢ncia sem√¢ntica vs sint√°tica

### Step 5: Context Enrichment
```python
# Prepara√ß√£o do contexto
context = "\n\n".join([doc.page_content for doc in relevant_docs])
```
**Conceitos:** Otimiza√ß√£o de context window

### Step 6: Answer Generation
```python
# Gera√ß√£o com contexto
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)
response = llm.invoke([SystemMessage(context), HumanMessage(query)])
```
**Conceitos:** Prompt engineering para RAG

## Como Usar

### 1. Configura√ß√£o Inicial

```bash
# 1. Instalar Ollama (Opcional para uso local)
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mxbai-embed-large:latest
ollama pull mistral:latest

# 2. Instalar depend√™ncias Python
pip install -r requirements.txt

# 3. Configurar APIs (opcional)
export OPENAI_API_KEY="sua-chave-openai"
export TAVILY_API_KEY="sua-chave-tavily"  # Para busca web
```

### 2. Executar Pipeline Completo

```bash
# RAG completo com interface interativa
python RAG_pipeline.py

# Escolher durante execu√ß√£o:
# 1. Apenas Ollama (100% local, gratuito)
# 2. Ollama + OpenAI (embeddings locais, gera√ß√£o OpenAI)
```

### 3. Explorar Conceitos Individuais

```bash
# Entender chunking
python chunking-example.py

# Ver embeddings em a√ß√£o
python embedding-example.py

# Comparar busca sem√¢ntica vs tradicional
python semantic-search-example.py

# Analisar enriquecimento de contexto
python context_enrichment.py
```

## Tecnologias e Modelos

### Modelos de Embedding
| Modelo | Dimens√µes | Idioma | Velocidade | Qualidade |
|--------|-----------|--------|------------|-----------|
| **mxbai-embed-large** | 1024 | Multilingual | üöÄ R√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **text-embedding-3-small** | 1536 | Multilingual | üåê API | ‚≠ê‚≠ê‚≠ê‚≠ê |

### Vector Stores
| Store | Uso | Performance | Persist√™ncia |
|-------|-----|-------------|--------------|
| **Chroma** | Desenvolvimento | üöÄ R√°pido | ‚úÖ Local |
| **FAISS** | Produ√ß√£o | ‚ö° Ultra-r√°pido | ‚úÖ Local |

### LLMs Suportados
- **Ollama Local**: mistral, llama2, deepseek-r1
- **OpenAI**: gpt-3.5-turbo, gpt-4
- **Configur√°vel**: Temperatura, max_tokens, etc.

## Par√¢metros de Otimiza√ß√£o

### Chunking Strategy
```python
# Configura√ß√£o recomendada
chunk_size = 1000        # Balan√ßo contexto/precis√£o
chunk_overlap = 200      # 20% overlap para continuidade
separators = ["\n\n", "\n", " "]  # Hier√°rquica
```

### Retrieval Configuration
```python
# Busca otimizada
search_type = "similarity"    # vs "mmr" (diversidade)
k = 5                        # N√∫mero de chunks
score_threshold = 0.7        # Filtro de relev√¢ncia
```

### Generation Settings
```python
# LLM para gera√ß√£o
temperature = 0.3            # Balan√ßo precis√£o/criatividade
max_tokens = 1000           # Limite de resposta
top_p = 0.9                 # Nucleus sampling
```

## An√°lise de Performance

### M√©tricas de Qualidade

**Retrieval Metrics:**
- **Precision@k**: Relev√¢ncia dos top-k chunks
- **Recall@k**: Cobertura de informa√ß√£o relevante
- **MRR**: Mean Reciprocal Rank

**Generation Metrics:**
- **Faithfulness**: Fidelidade ao contexto recuperado
- **Relevancy**: Relev√¢ncia contextual da resposta
- **Correctness**: Precis√£o factual

### Debugging RAG
**Problemas Comuns e Solu√ß√µes:**
1. **Chunks Irrelevantes**
   ```python
   # Solu√ß√£o: Ajustar chunking
   chunk_size = 1500  # Mais contexto
   chunk_overlap = 300  # Mais continuidade
   ```

2. **Contexto Insuficiente** 
   ```python
   # Solu√ß√£o: Mais chunks
   search_kwargs = {"k": 8}  # Mais resultados
   ```

3. **Respostas Inconsistentes**
   ```python
   # Solu√ß√£o: Menor temperatura
   temperature = 0.1  # Mais determin√≠stico
   ```

## Casos de Uso Ideais
### √ìtimo para RAG
- **Documenta√ß√£o t√©cnica**: APIs, manuais, guias
- **Base de conhecimento**: FAQ, pol√≠ticas, procedimentos  
- **Pesquisa acad√™mica**: Papers, relat√≥rios, estudos
- **Conte√∫do empresarial**: Relat√≥rios, apresenta√ß√µes

### Limita√ß√µes do RAG
- **Informa√ß√µes desatualizadas**: RAG √© est√°tico
- **Racioc√≠nio complexo**: Melhor usar fine-tuning
- **Dados estruturados**: Considerar SQL/GraphRAG
- **Tempo real**: Informa√ß√µes din√¢micas

## Experimentos e Extens√µes
### T√©cnicas Avan√ßadas (Para Estudo)
1. **Advanced Chunking**:
   - Semantic chunking (por t√≥picos)
   - Document-aware splitting
   - Hierarchical chunking

2. **Retrieval Enhancement**:
   - Hybrid search (sem√¢ntica + palavra-chave)
   - Reranking models
   - Query expansion/reformulation

3. **Context Optimization**:
   - Context compression
   - Relevant sentence extraction
   - Multi-hop reasoning

4. **Evaluation Framework**:
   - Human evaluation
   - A/B testing
   - Domain-specific metrics

## Conceitos Demonstrados
### Para Estudantes
- **Vector databases**: Armazenamento e busca eficiente
- **Embedding models**: Representa√ß√£o sem√¢ntica de texto
- **Similarity search**: Recupera√ß√£o por significado
- **Context window management**: Otimiza√ß√£o de prompt
- **Local vs Cloud**: Trade-offs Ollama vs OpenAI
- **Evaluation frameworks**: M√©tricas de qualidade

### Para Pesquisadores
- **Information retrieval**: T√©cnicas de recupera√ß√£o
- **Neural embeddings**: Representa√ß√µes distribu√≠das
- **Prompt engineering**: Otimiza√ß√£o de instru√ß√µes
- **Performance optimization**: Tuning de hiperpar√¢metros
